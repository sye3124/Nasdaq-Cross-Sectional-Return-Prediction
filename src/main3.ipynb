{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c719bc81",
   "metadata": {},
   "source": [
    "# Main Pipeline (Monthly Return Prediction + Portfolio Backtest)\n",
    "\n",
    "This notebook runs the full end-to-end pipeline:\n",
    "\n",
    "1. Load monthly returns, characteristics, and factor data  \n",
    "2. Compute factor exposures (betas / alpha)  \n",
    "3. Assemble the final modeling panel (features + next_return)  \n",
    "4. Generate rolling out-of-sample predictions (OLS, Ridge, Lasso, ENet, RF)  \n",
    "5. Build decile portfolios and long–short spreads  \n",
    "6. Produce performance tables, significance tests, and plots\n",
    "\n",
    "All logic lives in the project modules; this notebook is only orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modules\n",
    "from final_dataset import assemble_modeling_dataset\n",
    "from oos_predictions import generate_oos_predictions_all_models\n",
    "from portfolios import compute_decile_portfolio_returns, compute_decile_portfolio_weights\n",
    "from reporting import generate_comparison_report\n",
    "from training_scheme import WindowConfig\n",
    "\n",
    "# If you have this module in your repo:\n",
    "# from factor_exposures import compute_factor_exposures\n",
    "\n",
    "DIR_DATA = Path(\"..\") / \"data\"\n",
    "DIR_RESULTS = Path(\"..\") / \"results\"\n",
    "DIR_REPORTS = DIR_RESULTS / \"reports\"\n",
    "DIR_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "DIR_REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RETURN_COL = \"RET\"               # realized monthly return column in returns df\n",
    "TARGET_COL = \"next_return\"       # target col inside panel\n",
    "REALIZED_COL = \"realized_return\" # realized col name in predictions output\n",
    "\n",
    "STANDARDIZE = \"zscore\"           # \"zscore\" or \"rank\"\n",
    "\n",
    "window_cfg = WindowConfig(\n",
    "    min_train_months=60,\n",
    "    max_train_months=120,\n",
    "    expanding=False,\n",
    ")\n",
    "\n",
    "N_DECILES = 10\n",
    "WEIGHT_COL = None  # set to e.g. \"mktcap\" if you have it in predictions panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc59f1d",
   "metadata": {},
   "source": [
    "# 1) Load a 10% sample of tickers (fast dev mode)\n",
    "\n",
    "We load a deterministic 10% subset of tickers from metadata, then read only:\n",
    "- `date`\n",
    "- `adj_close`\n",
    "- `volume`\n",
    "\n",
    "We also cap the maximum rows per ticker to keep the run light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# --- Paths (match your setup) ---\n",
    "DIR_DATA = Path(\"../data\")\n",
    "DIR_STOCK_DATA = DIR_DATA / \"stock_data\"\n",
    "PATH_META = DIR_DATA / \"metadata_listed.csv\"\n",
    "PATH_META_DELISTED = DIR_DATA / \"metadata_delisted.csv\"\n",
    "\n",
    "# --- Controls ---\n",
    "P_SAMPLE = 0.10              # 10% of tickers\n",
    "MAX_ROWS_PER_TICKER = 1500   # reduce further (e.g. 500) if heavy\n",
    "PRICE_COL = \"adj_close\"\n",
    "VOLUME_COL = \"volume\"\n",
    "\n",
    "# Read metadata\n",
    "df_meta = pd.read_csv(PATH_META)\n",
    "df_meta_delisted = pd.read_csv(PATH_META_DELISTED)\n",
    "\n",
    "# Combine ticker universe\n",
    "tickers = pd.concat(\n",
    "    [df_meta[[\"Symbol\"]], df_meta_delisted[[\"Symbol\"]]],\n",
    "    ignore_index=True\n",
    ")[\"Symbol\"].dropna().astype(str).unique()\n",
    "\n",
    "# Deterministic sampling (stable across runs *in the same Python environment*)\n",
    "# If you want it stable across machines/Python versions, see note below.\n",
    "def keep_ticker(t: str, p: float) -> bool:\n",
    "    return (hash(t) % 10_000) / 10_000 < p\n",
    "\n",
    "tickers_sample = [t for t in tickers if keep_ticker(t, P_SAMPLE)]\n",
    "logging.info(f\"Selected {len(tickers_sample)} tickers out of {len(tickers)} (~{P_SAMPLE:.0%}).\")\n",
    "\n",
    "all_stocks = []\n",
    "skipped = 0\n",
    "\n",
    "usecols = [\"date\", PRICE_COL, VOLUME_COL]\n",
    "\n",
    "for symbol in tickers_sample:\n",
    "    path_stock = DIR_STOCK_DATA / f\"{symbol}.csv\"\n",
    "    if not path_stock.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_stock = pd.read_csv(\n",
    "            path_stock,\n",
    "            usecols=lambda c: c in usecols,\n",
    "            parse_dates=[\"date\"],\n",
    "        )\n",
    "\n",
    "        if MAX_ROWS_PER_TICKER is not None and len(df_stock) > MAX_ROWS_PER_TICKER:\n",
    "            df_stock = df_stock.tail(MAX_ROWS_PER_TICKER)\n",
    "\n",
    "        df_stock = df_stock.sort_values(\"date\")\n",
    "\n",
    "        # Downcast for memory\n",
    "        df_stock[PRICE_COL] = pd.to_numeric(df_stock[PRICE_COL], errors=\"coerce\", downcast=\"float\")\n",
    "        df_stock[VOLUME_COL] = pd.to_numeric(df_stock[VOLUME_COL], errors=\"coerce\", downcast=\"float\")\n",
    "\n",
    "        df_stock[\"ticker\"] = symbol\n",
    "        df_stock = df_stock.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "\n",
    "        all_stocks.append(df_stock)\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        logging.warning(f\"Skipping {symbol}: {e}\")\n",
    "\n",
    "df_stocks = pd.concat(all_stocks) if all_stocks else pd.DataFrame()\n",
    "logging.info(f\"Loaded panel shape: {df_stocks.shape}, skipped: {skipped}, rows: {len(df_stocks):,}\")\n",
    "\n",
    "df_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Run the full pipeline on the sample\n",
    "\n",
    "Steps:\n",
    "1. Compute monthly features (characteristics)\n",
    "2. Compute factor exposures (betas / alpha)\n",
    "3. Assemble modeling dataset (panel with next_return)\n",
    "4. Generate OOS predictions\n",
    "5. Compute decile portfolio returns (and optionally weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from features import FeatureConfig, compute_features\n",
    "from factor_exposures import ExposureConfig, compute_factor_exposures\n",
    "from final_dataset import assemble_modeling_dataset\n",
    "from oos_predictions import generate_oos_predictions_all_models\n",
    "from portfolios import compute_decile_portfolio_returns, compute_decile_portfolio_weights\n",
    "from training_scheme import WindowConfig\n",
    "\n",
    "# Output\n",
    "DIR_OUTPUT = Path(\"outputs\")\n",
    "DIR_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configs\n",
    "feature_cfg = FeatureConfig()\n",
    "feature_cfg.price_col = getattr(feature_cfg, \"price_col\", \"adj_close\")\n",
    "feature_cfg.volume_col = getattr(feature_cfg, \"volume_col\", \"volume\")\n",
    "\n",
    "exposure_cfg = ExposureConfig()\n",
    "window_cfg = WindowConfig()\n",
    "\n",
    "standardize_method = \"zscore\"   # or \"rank\"\n",
    "\n",
    "# Input: sampled daily panel\n",
    "stock_data = df_stocks\n",
    "\n",
    "# --- 1) Compute features (monthly characteristics) ---\n",
    "logging.info(\"Computing features...\")\n",
    "characteristics = compute_features(stock_data, config=feature_cfg)\n",
    "logging.info(f\"Characteristics shape: {characteristics.shape}\")\n",
    "\n",
    "# --- 2) Compute factor exposures (monthly betas/alpha) ---\n",
    "logging.info(\"Computing factor exposures...\")\n",
    "factor_loadings = compute_factor_exposures(characteristics, config=exposure_cfg)\n",
    "logging.info(f\"Factor loadings shape: {factor_loadings.shape}\")\n",
    "\n",
    "# --- 3) Assemble final modeling dataset ---\n",
    "# assemble_modeling_dataset expects:\n",
    "# - returns: MultiIndex ('ticker','date') with column RET (monthly realized)\n",
    "# - characteristics: MultiIndex ('ticker','date') numeric features\n",
    "# - factor_loadings: MultiIndex ('ticker','date') rolling betas\n",
    "#\n",
    "# IMPORTANT: compute_features must return (or include) a monthly return column for you to build `returns`.\n",
    "# Most pipelines either:\n",
    "#   - include RET in the output already, OR\n",
    "#   - you compute monthly returns separately from adj_close.\n",
    "\n",
    "if \"RET\" in characteristics.columns:\n",
    "    returns = characteristics[[\"RET\"]].copy()\n",
    "else:\n",
    "    raise KeyError(\n",
    "        \"Couldn't find 'RET' in characteristics. \"\n",
    "        \"Either modify compute_features to output monthly RET, \"\n",
    "        \"or compute monthly returns from adj_close and build returns DataFrame.\"\n",
    "    )\n",
    "\n",
    "# Remove RET from characteristics if you don't want it treated as a feature\n",
    "characteristics_only = characteristics.drop(columns=[\"RET\"], errors=\"ignore\")\n",
    "\n",
    "logging.info(\"Assembling modeling dataset...\")\n",
    "panel = assemble_modeling_dataset(\n",
    "    returns=returns,\n",
    "    characteristics=characteristics_only,\n",
    "    factor_loadings=factor_loadings,\n",
    "    return_col=\"RET\",\n",
    "    next_return_col=\"next_return\",\n",
    "    standardize=standardize_method,\n",
    ")\n",
    "\n",
    "logging.info(f\"Final panel shape: {panel.shape}\")\n",
    "panel.to_csv(DIR_OUTPUT / \"panel_sample.csv\")\n",
    "\n",
    "# --- 4) OOS predictions ---\n",
    "feature_cols = [c for c in panel.columns if c != \"next_return\"]\n",
    "\n",
    "logging.info(\"Generating OOS predictions...\")\n",
    "predictions = generate_oos_predictions_all_models(\n",
    "    panel,\n",
    "    feature_cols,\n",
    "    target_col=\"next_return\",\n",
    "    window_config=window_cfg,\n",
    "    output_path=DIR_OUTPUT / \"predictions_sample.csv\",\n",
    "    realized_col=\"realized_return\",\n",
    ")\n",
    "\n",
    "logging.info(f\"Predictions shape: {predictions.shape}\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Decile portfolios (sample run)\n",
    "\n",
    "We form deciles each month based on predictions and compute next-month realized returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42becffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Computing decile portfolio returns...\")\n",
    "decile_returns = compute_decile_portfolio_returns(\n",
    "    predictions,\n",
    "    return_col=\"realized_return\",\n",
    "    n_deciles=10,\n",
    "    weight_col=None,   # set to a market-cap column if you have one\n",
    ")\n",
    "decile_returns.to_csv(DIR_OUTPUT / \"decile_returns_sample.csv\")\n",
    "\n",
    "logging.info(\"Computing decile portfolio weights...\")\n",
    "decile_weights = compute_decile_portfolio_weights(\n",
    "    predictions,\n",
    "    n_deciles=10,\n",
    "    weight_col=None,\n",
    ")\n",
    "decile_weights.to_csv(DIR_OUTPUT / \"decile_weights_sample.csv\")\n",
    "\n",
    "decile_returns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Assemble final modeling panel\n",
    "\n",
    "This creates the final dataset with:\n",
    "- standardized characteristics\n",
    "- factor loadings\n",
    "- `next_return` target (shifted by ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = assemble_modeling_dataset(\n",
    "    returns=returns,\n",
    "    characteristics=characteristics,\n",
    "    factor_loadings=factor_loadings,\n",
    "    return_col=RETURN_COL,\n",
    "    next_return_col=TARGET_COL,\n",
    "    standardize=STANDARDIZE,\n",
    ")\n",
    "\n",
    "panel.to_csv(DIR_RESULTS / \"panel.csv\")\n",
    "panel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Rolling out-of-sample predictions (multiple models)\n",
    "\n",
    "This generates one prediction column per model + realized next return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in panel.columns if c != TARGET_COL]\n",
    "\n",
    "predictions = generate_oos_predictions_all_models(\n",
    "    panel,\n",
    "    feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    window_config=window_cfg,\n",
    "    realized_col=REALIZED_COL,\n",
    "    output_path=DIR_RESULTS / \"predictions.csv\",\n",
    ")\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Decile portfolios + long–short\n",
    "\n",
    "We sort stocks each month into deciles based on each model’s prediction,\n",
    "then compute the next-month realized return for each decile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_returns = compute_decile_portfolio_returns(\n",
    "    predictions,\n",
    "    return_col=REALIZED_COL,\n",
    "    model_cols=None,\n",
    "    weight_col=WEIGHT_COL,\n",
    "    n_deciles=N_DECILES,\n",
    ")\n",
    "decile_returns.to_csv(DIR_RESULTS / \"decile_returns.csv\")\n",
    "\n",
    "decile_weights = compute_decile_portfolio_weights(\n",
    "    predictions,\n",
    "    model_cols=None,\n",
    "    weight_col=WEIGHT_COL,\n",
    "    n_deciles=N_DECILES,\n",
    ")\n",
    "decile_weights.to_csv(DIR_RESULTS / \"decile_weights.csv\")\n",
    "\n",
    "decile_returns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Reporting (performance table, significance tests, plots)\n",
    "\n",
    "This will output:\n",
    "- performance summary table\n",
    "- DM test table (forecast accuracy differences)\n",
    "- Sharpe significance tests\n",
    "- rolling Sharpe plot\n",
    "- cumulative return plot\n",
    "- factor exposure plot (if factor loadings + weights exist)\n",
    "- feature importance plot (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = generate_comparison_report(\n",
    "    decile_returns=decile_returns,\n",
    "    prediction_panel=predictions,\n",
    "    portfolio_weights=decile_weights,\n",
    "    factor_loadings=factor_loadings,\n",
    "    feature_panel=panel,\n",
    "    feature_cols=feature_cols,\n",
    "    transaction_cost_bps=None,  # set e.g. 10 for 10 bps cost per unit turnover\n",
    "    realized_col=REALIZED_COL,\n",
    "    risk_free_rate=0.0,\n",
    "    periods_per_year=12,\n",
    "    rolling_sharpe_window=12,\n",
    "    output_dir=DIR_REPORTS,\n",
    ")\n",
    "\n",
    "# Save key outputs\n",
    "if isinstance(report.get(\"performance_metrics\"), pd.DataFrame):\n",
    "    report[\"performance_metrics\"].to_csv(DIR_REPORTS / \"performance_metrics.csv\")\n",
    "\n",
    "if isinstance(report.get(\"dm_table\"), pd.DataFrame):\n",
    "    report[\"dm_table\"].to_csv(DIR_REPORTS / \"dm_table.csv\")\n",
    "\n",
    "if isinstance(report.get(\"sharpe_table\"), pd.DataFrame):\n",
    "    report[\"sharpe_table\"].to_csv(DIR_REPORTS / \"sharpe_table.csv\")\n",
    "\n",
    "report[\"performance_metrics\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- `results/panel.csv`\n",
    "- `results/predictions.csv`\n",
    "- `results/decile_returns.csv`\n",
    "- `results/decile_weights.csv`\n",
    "- `results/reports/*` (tables + plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions with realized returns\n",
    "panel_with_preds = panel.join([df_ols_pred, df_ridge_pred], how='left')\n",
    "panel_with_preds = panel_with_preds.rename(columns={'next_return': 'realized_return'})\n",
    "\n",
    "ranking_cfg = RankingConfig(prediction_col='ridge_prediction', output_col='ridge_rank', basis='zscore')\n",
    "df_ridge_rank = convert_predictions_to_rankings(panel_with_preds[['ridge_prediction']], config=ranking_cfg)\n",
    "panel_ranked = panel_with_preds.join(df_ridge_rank)\n",
    "\n",
    "decile_returns = compute_decile_portfolio_returns(\n",
    "    panel_ranked,\n",
    "    model_cols=['ridge_rank', 'ols_prediction'],\n",
    "    return_col='realized_return',\n",
    ")\n",
    "decile_weights = compute_decile_portfolio_weights(\n",
    "    panel_ranked,\n",
    "    model_cols=['ridge_rank', 'ols_prediction'],\n",
    ")\n",
    "decile_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 — Performance summary\n",
    "Compute annualized performance stats, cumulative returns, and drawdowns for each model/decile combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, cumulative, drawdowns = summarize_portfolio_performance(\n",
    "    decile_returns,\n",
    "    turnover_weights=decile_weights,\n",
    "    transaction_cost_bps=0,\n",
    "    periods_per_year=12,\n",
    ")\n",
    "metrics.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
